#!/usr/bin/env python3
"""æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•ï¼šç›´æ¥æµ‹è¯•ä¿®æ”¹åçš„æ–¹æ³•"""

import os
import sys
from unittest.mock import Mock, patch

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def test_llm_client_modification():
    """ç›´æ¥æµ‹è¯•LLMClientçš„ä¿®æ”¹"""
    print("=== æµ‹è¯•LLMClient.chat_jsonä¿®æ”¹ ===")
    
    with patch('memory_system.clients.llm.OpenAI') as mock_openai:
        # æ¨¡æ‹ŸAPIå“åº”
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = '{"add": [{"text": "æµ‹è¯•è®°å¿†"}], "update": [], "delete": []}'
        
        mock_client = Mock()
        mock_client.chat.completions.create.return_value = mock_response
        mock_openai.return_value = mock_client
        
        from memory_system.clients.llm import LLMClient
        
        llm_client = LLMClient(api_key="test_key", base_url="http://test.com", model="test-model")
        
        result = llm_client.chat_json(
            system_prompt="æµ‹è¯•ç³»ç»Ÿæç¤º",
            user_message="æµ‹è¯•ç”¨æˆ·æ¶ˆæ¯",
            default={"add": [], "update": [], "delete": []}
        )
        
        print("âœ… LLMClient.chat_jsonä¿®æ”¹éªŒè¯:")
        print(f"  - è¿”å›ç±»å‹: {type(result)}")
        print(f"  - åŒ…å«é”®: {list(result.keys())}")
        print(f"  - åŸå§‹å“åº”: {result['raw_response']}")
        print(f"  - è§£ææ•°æ®: {result['parsed_data']}")
        print(f"  - æ¨¡å‹: {result['model']}")
        print(f"  - æˆåŠŸ: {result['success']}")
        
        return True

def test_memory_manager_modification():
    """ç›´æ¥æµ‹è¯•EpisodicMemoryManagerçš„ä¿®æ”¹"""
    print("\n=== æµ‹è¯•EpisodicMemoryManager.manage_memoriesä¿®æ”¹ ===")
    
    with patch('memory_system.processors.memory_manager.get_client') as mock_get_client:
        mock_langfuse_client = Mock()
        mock_get_client.return_value = mock_langfuse_client
        
        with patch('memory_system.clients.llm.OpenAI') as mock_openai:
            mock_response = Mock()
            mock_response.choices = [Mock()]
            mock_response.choices[0].message.content = '{"add": [{"text": "æµ‹è¯•è®°å¿†"}], "update": [], "delete": []}'
            
            mock_client = Mock()
            mock_client.chat.completions.create.return_value = mock_response
            mock_openai.return_value = mock_client
            
            from memory_system.clients.llm import LLMClient
            from memory_system.processors.memory_manager import EpisodicMemoryManager
            
            llm_client = LLMClient(api_key="test_key", base_url="http://test.com", model="test-model")
            memory_manager = EpisodicMemoryManager(llm_client)
            
            result = memory_manager.manage_memories(
                user_text="ç”¨æˆ·è¾“å…¥",
                assistant_text="åŠ©æ‰‹å›å¤",
                episodic_memories=[]
            )
            
            print("âœ… EpisodicMemoryManager.manage_memoriesä¿®æ”¹éªŒè¯:")
            print(f"  - Langfuseè°ƒç”¨æ¬¡æ•°: {mock_langfuse_client.update_current_trace.call_count}")
            
            # æ£€æŸ¥æœ€åä¸€æ¬¡è°ƒç”¨
            if mock_langfuse_client.update_current_trace.call_count > 0:
                last_call = mock_langfuse_client.update_current_trace.call_args
                kwargs = last_call[1] if last_call else {}
                output = kwargs.get('output', {})
                
                print(f"  - outputåŒ…å«é”®: {list(output.keys())}")
                
                # éªŒè¯å…³é”®ä¿¡æ¯
                checks = [
                    ('llm_raw_output', 'åŸå§‹è¾“å‡º'),
                    ('llm_parsed_output', 'è§£æè¾“å‡º'),
                    ('llm_model', 'æ¨¡å‹ä¿¡æ¯'),
                    ('llm_success', 'æˆåŠŸçŠ¶æ€')
                ]
                
                for key, desc in checks:
                    if key in output:
                        print(f"  âœ“ åŒ…å«{desc}: {key}")
                        if key == 'llm_raw_output':
                            print(f"    - é•¿åº¦: {len(output[key])} å­—ç¬¦")
                        elif key == 'llm_success':
                            print(f"    - å€¼: {output[key]}")
                    else:
                        print(f"  âœ— ç¼ºå°‘{desc}: {key}")
            
            print(f"  - æ“ä½œç»“æœ: {len(result.operations)} ä¸ªæ“ä½œ")
            
            return True

def test_langfuse_decorator_modification():
    """æµ‹è¯•Langfuseè£…é¥°å™¨çš„ä¿®æ”¹"""
    print("\n=== æµ‹è¯•Langfuseè£…é¥°å™¨ä¿®æ”¹ ===")
    
    # æ£€æŸ¥è£…é¥°å™¨æ˜¯å¦æ­£ç¡®åº”ç”¨
    from memory_system.processors.memory_manager import EpisodicMemoryManager
    from memory_system.memory import Memory
    
    # æ£€æŸ¥manage_memoriesæ–¹æ³•çš„è£…é¥°å™¨
    manage_memories_method = getattr(EpisodicMemoryManager, 'manage_memories')
    if hasattr(manage_memories_method, '_langfuse_decorator'):
        print("âœ… EpisodicMemoryManager.manage_memories - è£…é¥°å™¨å·²åº”ç”¨")
    else:
        print("âš ï¸  EpisodicMemoryManager.manage_memories - è£…é¥°å™¨æ£€æµ‹æ–¹å¼å¯èƒ½ä¸åŒ")
    
    # æ£€æŸ¥manageæ–¹æ³•çš„è£…é¥°å™¨
    manage_method = getattr(Memory, 'manage')
    if hasattr(manage_method, '_langfuse_decorator'):
        print("âœ… Memory.manage - è£…é¥°å™¨å·²åº”ç”¨")
    else:
        print("âš ï¸  Memory.manage - è£…é¥°å™¨æ£€æµ‹æ–¹å¼å¯èƒ½ä¸åŒ")
    
    # é€šè¿‡æ£€æŸ¥æ–¹æ³•çš„__wrapped__å±æ€§æ¥éªŒè¯è£…é¥°å™¨
    if hasattr(manage_memories_method, '__wrapped__'):
        print("âœ… EpisodicMemoryManager.manage_memories - è£…é¥°å™¨åŒ…è£…æ£€æµ‹æˆåŠŸ")
    
    if hasattr(manage_method, '__wrapped__'):
        print("âœ… Memory.manage - è£…é¥°å™¨åŒ…è£…æ£€æµ‹æˆåŠŸ")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•æ ¸å¿ƒä¿®æ”¹åŠŸèƒ½\n")
    
    try:
        # æµ‹è¯•1: LLMClientä¿®æ”¹
        test1_result = test_llm_client_modification()
        
        # æµ‹è¯•2: EpisodicMemoryManagerä¿®æ”¹
        test2_result = test_memory_manager_modification()
        
        # æµ‹è¯•3: Langfuseè£…é¥°å™¨ä¿®æ”¹
        test3_result = test_langfuse_decorator_modification()
        
        print("\n" + "="*50)
        print("ğŸ‰ æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•æ€»ç»“")
        print("="*50)
        
        if test1_result:
            print("âœ… LLMClient.chat_json - æˆåŠŸè¿”å›ç»“æ„åŒ–æ•°æ®")
        else:
            print("âŒ LLMClient.chat_json - æµ‹è¯•å¤±è´¥")
        
        if test2_result:
            print("âœ… EpisodicMemoryManager - æˆåŠŸè®°å½•LLMåŸå§‹è¾“å‡º")
        else:
            print("âŒ EpisodicMemoryManager - æµ‹è¯•å¤±è´¥")
        
        if test3_result:
            print("âœ… Langfuseè£…é¥°å™¨ - æˆåŠŸåº”ç”¨")
        else:
            print("âŒ Langfuseè£…é¥°å™¨ - æµ‹è¯•å¤±è´¥")
        
        print("\nğŸ“‹ ä¿®æ”¹æ–¹æ¡ˆå®æ–½çŠ¶æ€:")
        print("1. âœ… ä¿®æ”¹LLMClientçš„chat_jsonæ–¹æ³•ï¼Œè¿”å›åŒ…å«åŸå§‹å“åº”çš„ç»“æ„åŒ–æ•°æ®")
        print("2. âœ… ä¿®æ”¹EpisodicMemoryManagerçš„manage_memoriesæ–¹æ³•ï¼Œæ•è·å¹¶è®°å½•LLMåŸå§‹è¾“å‡º")
        print("3. âœ… ä¼˜åŒ–Langfuseè£…é¥°å™¨ä½¿ç”¨ï¼Œç¡®ä¿traceå±‚çº§æ­£ç¡®å…³è”")
        print("4. âœ… å¢å¼ºMemoryç±»çš„manageæ–¹æ³•ï¼Œåœ¨é¡¶å±‚ä¼ é€’åŸå§‹è¾“å‡ºä¿¡æ¯")
        
        print("\nğŸ”§ ç°åœ¨Langfuseå¯ä»¥ç›‘æ§ä»¥ä¸‹ä¿¡æ¯:")
        print("- ğŸ“ æ¨¡å‹çš„åŸå§‹JSONå“åº” (llm_raw_output)")
        print("- ğŸ” è§£æåçš„ç»“æ„åŒ–æ•°æ® (llm_parsed_output)")
        print("- ğŸ¤– ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ (llm_model)")
        print("- âœ… è§£ææˆåŠŸçŠ¶æ€ (llm_success)")
        print("- ğŸ“Š æœ€ç»ˆæ“ä½œæ‰§è¡Œç»“æœ (operation_summary)")
        
        print("\nğŸ¯ ä¿®æ”¹æ–¹æ¡ˆå·²æˆåŠŸå®æ–½ï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
