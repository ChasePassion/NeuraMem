"""
Gradio-based visualization demo for AI Memory System.

Features:
- Left panel: Real-time memory display (episodic + semantic)
- Right panel: Chat interface
- Memory consolidation with progress display
- Scheduled consolidation support
"""

import gradio as gr
import asyncio
import time
import json
import threading
import logging
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import sys
import os
from langfuse import observe, get_client
# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.memory_system import Memory, MemoryConfig, MemoryRecord, ConsolidationStats


# Setup logger
logger = logging.getLogger(__name__)


class MemoryDemoApp:
    """Main demo application class."""
    
    def __init__(self):
        """Initialize the demo application."""
        self.memory: Optional[Memory] = None
        self.current_user_id: str = "demo_user"
        self.chat_history: List[Dict[str, str]] = []
        self.consolidation_log: List[str] = []
        self.is_consolidating: bool = False
        self.scheduled_task: Optional[threading.Timer] = None
        
    def initialize_memory_system(self, user_id: str) -> str:
        """Initialize or reinitialize the memory system."""
        try:
            if not user_id.strip():
                user_id = "demo_user"
            self.current_user_id = user_id.strip()
            
            config = MemoryConfig()
            config.collection_name = f"demo_memories_{self.current_user_id}"
            self.memory = Memory(config)
            self.chat_history = []
            self.consolidation_log = []
            
            return f"âœ… è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼ç”¨æˆ·ID: {self.current_user_id}"
        except Exception as e:
            return f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}"
    
    def get_all_memories(self) -> str:
        """Get all memories for current user and format as display text."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"
        
        try:
            # Query episodic memories
            episodic = self.memory._store.query(
                filter_expr=f'user_id == "{self.current_user_id}" and memory_type == "episodic"',
                output_fields=["id", "text", "ts"],
                limit=100
            )
            
            # Query semantic memories
            semantic = self.memory._store.query(
                filter_expr=f'user_id == "{self.current_user_id}" and memory_type == "semantic"',
                output_fields=["id", "text", "ts"],
                limit=100
            )
            
            output = []
            output.append(f"ğŸ“Š è®°å¿†ç»Ÿè®¡ - ç”¨æˆ·: {self.current_user_id}")
            output.append(f"{'='*50}")
            output.append(f"æƒ…æ™¯è®°å¿†: {len(episodic)} æ¡ | è¯­ä¹‰è®°å¿†: {len(semantic)} æ¡")
            output.append("")
            
            # Display episodic memories
            output.append("ğŸ¬ æƒ…æ™¯è®°å¿† (Episodic)")
            output.append("-" * 40)
            if episodic:
                for mem in sorted(episodic, key=lambda x: x.get("ts", 0), reverse=True):
                    ts = mem.get("ts", 0)
                    time_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M") if ts else "N/A"
                    text = mem.get("text", "")
                    output.append(f"[ID:{mem.get('id')}] æ—¶é—´:{time_str}")
                    output.append(f"  å†…å®¹: {text}")
                    output.append("")
            else:
                output.append("  (æš‚æ— æƒ…æ™¯è®°å¿†)")
                output.append("")
            
            # Display semantic memories
            output.append("ğŸ§  è¯­ä¹‰è®°å¿† (Semantic)")
            output.append("-" * 40)
            if semantic:
                for mem in sorted(semantic, key=lambda x: x.get("ts", 0), reverse=True):
                    ts = mem.get("ts", 0)
                    time_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M") if ts else "N/A"
                    fact = mem.get("text", "")
                    output.append(f"[ID:{mem.get('id')}] æ—¶é—´:{time_str}")
                    output.append(f"  å†…å®¹: {fact}")
                    output.append("")
            else:
                output.append("  (æš‚æ— è¯­ä¹‰è®°å¿†)")
            
            return "\n".join(output)
            
        except Exception as e:
            return f"âŒ è·å–è®°å¿†å¤±è´¥: {str(e)}"

    @observe(as_type="agent") 
    async def chat(self, message: str, history: List[Any]) -> Tuple[str, List[Dict[str, str]], str]:
        """Process chat message with intelligent reconsolidation: search â†’ respond â†’ judge usage â†’ reconsolidate used memories."""
        history_messages = self._normalize_history(history)
    
        get_client().update_current_trace(
            session_id=f"demo_chat_{self.current_user_id}_{int(time.time())}",
            user_id=self.current_user_id,
            tags=["demo_chat", "memory_system"],
            metadata={
                "app": "MemoryDemoApp",
                "message_length": len(message),
                "history_length": len(history_messages)
            }
        )
        
        if not self.memory:
            return "", history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"}
            ], await asyncio.to_thread(self.get_all_memories)
        
        if not message.strip():
            return "", history_messages, await asyncio.to_thread(self.get_all_memories)
        
        try:
            # 1. å‡†å¤‡æ¶ˆæ¯å’Œä¸Šä¸‹æ–‡
            prepared_messages = self._prepare_messages(message, history_messages)
            
            # 2. æ£€ç´¢ç›¸å…³è®°å¿†
            relevant_memories = await asyncio.to_thread(
                self.memory.search,
                message,
                self.current_user_id,
                5
            )
            
            # 3. æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆä¼ å…¥ historyï¼‰
            full_context = self._build_context_with_memories(message, relevant_memories, history_messages)
            
            # 4. è°ƒç”¨LLMç”Ÿæˆå›å¤ï¼ˆæ”¾åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰
            ai_response = await asyncio.to_thread(self._generate_response, full_context, prepared_messages)
            
            # 5. è®°å¿†ç®¡ç†
            asyncio.create_task(self._manage_memory_async(message, ai_response, history_messages))
            
            # æ„å»ºæœ€ç»ˆå“åº”
            final_response = ai_response
            new_history = history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": final_response}
            ]
            return "", new_history, await asyncio.to_thread(self.get_all_memories)
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            return "", history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": error_msg}
            ], await asyncio.to_thread(self.get_all_memories)
    
    async def chat_stream(self, message: str, history: List[Any]):
        """Process chat message with streaming response and intelligent reconsolidation."""
        history_messages = self._normalize_history(history)
        
        if not self.memory:
            error_response = "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"
            new_history = history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": error_response}
            ]
            yield new_history, await asyncio.to_thread(self.get_all_memories)
            return
        
        if not message.strip():
            yield history_messages, await asyncio.to_thread(self.get_all_memories)
            return
        
        try:
            # 1. å‡†å¤‡æ¶ˆæ¯å’Œä¸Šä¸‹æ–‡
            prepared_messages = self._prepare_messages(message, history_messages)
            
            # 2. æ£€ç´¢ç›¸å…³è®°å¿†
            relevant_memories = await asyncio.to_thread(
                self.memory.search,
                message,
                self.current_user_id,
                5
            )
            
            # 3. æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆä¼ å…¥ historyï¼‰
            full_context = self._build_context_with_memories(message, relevant_memories, history_messages)
            
            # 4. åˆ›å»ºç”¨äºæ”¶é›†å®Œæ•´å›å¤çš„é˜Ÿåˆ—
            response_queue = asyncio.Queue()
            
            # 5. å¯åŠ¨æµå¼å“åº”ç”Ÿæˆ
            accumulated_response = ""
            new_history = history_messages + [{"role": "user", "content": message}]
            
            # å…ˆæ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            yield new_history, await asyncio.to_thread(self.get_all_memories)
            
            # æµå¼ç”Ÿæˆå›å¤
            async for chunk in self._generate_response_stream(full_context, prepared_messages):
                accumulated_response += chunk
                current_history = new_history + [{"role": "assistant", "content": accumulated_response}]
                yield current_history, await asyncio.to_thread(self.get_all_memories)
            
            # 6. å°†å®Œæ•´å›å¤æ”¾å…¥é˜Ÿåˆ—ä¾›è®°å¿†å¤„ç†ä½¿ç”¨
            await response_queue.put(accumulated_response)
            
            # 6. å¯åŠ¨è®°å¿†å†™å…¥ä»»åŠ¡ï¼ˆåœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼‰
            asyncio.create_task(self._manage_memory_async_with_queue(message, response_queue, history_messages))
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            error_history = history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": error_msg}
            ]
            yield error_history, await asyncio.to_thread(self.get_all_memories)
    
    def _normalize_history(self, history: List[Any]) -> List[Dict[str, str]]:
        """Normalize Chatbot history to the messages format Gradio expects."""
        normalized: List[Dict[str, str]] = []
        
        for item in history or []:
            if isinstance(item, dict) and "role" in item and "content" in item:
                normalized.append({"role": str(item["role"]), "content": str(item["content"])})
            elif hasattr(item, "role") and hasattr(item, "content"):
                role = getattr(item, "role", None)
                content = getattr(item, "content", None)
                if role is not None and content is not None:
                    normalized.append({"role": str(role), "content": str(content)})
            elif isinstance(item, (list, tuple)) and len(item) == 2:
                user_msg, ai_msg = item
                normalized.append({"role": "user", "content": str(user_msg)})
                normalized.append({"role": "assistant", "content": str(ai_msg)})
        
        return normalized
    
    def _history_pairs(self, history: List[Dict[str, str]]) -> List[Tuple[str, str]]:
        """Convert message-style history into user/assistant pairs for logging or prompts."""
        pairs: List[Tuple[str, str]] = []
        last_user: Optional[str] = None
        
        for msg in history:
            if msg.get("role") == "user":
                last_user = msg.get("content", "")
            elif msg.get("role") == "assistant" and last_user is not None:
                pairs.append((last_user, msg.get("content", "")))
                last_user = None
        
        return pairs
    
    def _prepare_messages(self, message: str, history: List[Dict[str, str]]) -> List[Dict]:
        """å‡†å¤‡å’Œæ ‡å‡†åŒ–æ¶ˆæ¯ï¼ŒåŒ…å«å†å²å¯¹è¯ä¸Šä¸‹æ–‡ã€‚"""
        messages = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in history[-50:]
            if isinstance(msg, dict) and "role" in msg and "content" in msg
        ]
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        messages.append({"role": "user", "content": message})
        
        return messages
    
    
    def _build_context_with_memories(self, message: str, memories: List[MemoryRecord], history: List[Dict[str, str]]) -> str:
        """æ„å»ºåŒ…å«è®°å¿†çš„å®Œæ•´ä¸Šä¸‹æ–‡ã€‚"""
        context_parts = []
        
        # åˆ†ç¦»æƒ…æ™¯è®°å¿†å’Œè¯­ä¹‰è®°å¿†
        episodic_memories = [mem for mem in memories if mem.memory_type == "episodic"]
        semantic_memories = [mem for mem in memories if mem.memory_type == "semantic"]
        history_pairs = self._history_pairs(history)
        
        # 1. æƒ…æ™¯è®°å¿†éƒ¨åˆ†
        context_parts.append("Here are the episodic memories:")
        if episodic_memories:
            for i, mem in enumerate(episodic_memories[:3], 1):
                context_parts.append(f"{i}. {mem.text}")
        else:
            context_parts.append("(No episodic memories)")
        context_parts.append("")
        
        # 2. è¯­ä¹‰è®°å¿†éƒ¨åˆ†
        context_parts.append("Here are the semantic memories:")
        if semantic_memories:
            for i, mem in enumerate(semantic_memories[:3], 1):
                context_parts.append(f"{i}. {mem.text}")
        else:
            context_parts.append("(No semantic memories)")
        context_parts.append("")
        
        # 3. å†å²å¯¹è¯éƒ¨åˆ†
        context_parts.append("Here are the history messages:")
        if history_pairs:
            for i, (user_msg, ai_msg) in enumerate(history_pairs[-3:], 1):
                context_parts.append(f"Turn {i}:")
                context_parts.append(f"  User: {user_msg}")
                context_parts.append(f"  Assistant: {ai_msg}")
        else:
            context_parts.append("(No history messages)")
        context_parts.append("")
        
        # 4. å½“å‰ä»»åŠ¡
        context_parts.append("Here are the task:")
        context_parts.append(message)
        
        return "\n".join(context_parts)
    
    def _generate_response(self, context: str, messages: List[Dict]) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆå›å¤ã€‚"""
        # å¯¼å…¥ MEMORY_ANSWER_PROMPT
        try:
            from prompts import MEMORY_ANSWER_PROMPT
            system_prompt = f"{MEMORY_ANSWER_PROMPT}\n\nUser ID: {self.current_user_id}\n\n{context}"
        except ImportError:
            system_prompt = f"""You are an AI assistant with long-term memory capabilities. User ID: {self.current_user_id}
Please answer based on the user's messages and relevant memories. If there are relevant memories, reflect that you remember the user's information in your response.
Maintain a friendly and natural conversation style.

{context}"""
        
        try:
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = messages[-1]["content"] if messages else ""
            ai_response = self.memory._llm_client.chat(system_prompt, user_message)
            return ai_response
        except Exception as llm_error:
            return f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤ã€‚é”™è¯¯: {str(llm_error)}"
    
    async def _generate_response_stream(self, context: str, messages: List[Dict]):
        """ä½¿ç”¨LLMæµå¼ç”Ÿæˆå›å¤ã€‚"""
        # å¯¼å…¥ MEMORY_ANSWER_PROMPT
        try:
            from prompts import MEMORY_ANSWER_PROMPT
            system_prompt = f"{MEMORY_ANSWER_PROMPT}\n\nUser ID: {self.current_user_id}\n\n{context}"
        except ImportError:
            system_prompt = f"""You are an AI assistant with long-term memory capabilities. User ID: {self.current_user_id}
Please answer based on the user's messages and relevant memories. If there are relevant memories, reflect that you remember the user's information in your response.
Maintain a friendly and natural conversation style.

{context}"""
        
        try:
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = messages[-1]["content"] if messages else ""
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæµå¼è°ƒç”¨
            response_stream = await asyncio.to_thread(
                self.memory._llm_client.chat_stream, system_prompt, user_message
            )
            
            accumulated_response = ""
            for chunk in response_stream:
                accumulated_response += chunk
                yield chunk
                
        except Exception as llm_error:
            yield f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤ã€‚é”™è¯¯: {str(llm_error)}"
    
    async def _manage_memory_async(self, user_message: str, assistant_message: str, history: List[Dict[str, str]]) -> None:
        """å¼‚æ­¥ç®¡ç†è®°å¿†åˆ°åå°ï¼ˆä¸é˜»å¡ Gradio äº‹ä»¶å¾ªç¯ï¼‰ã€‚"""
        try:
            chat_id = f"chat_{int(time.time())}"
            
            # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œæœªå®ç°æ—¶å›é€€åˆ°çº¿ç¨‹æ± å°è£…çš„åŒæ­¥æ¥å£
            if hasattr(self.memory, "manage_async"):
                await self.memory.manage_async(
                    user_text=user_message,
                    assistant_text=assistant_message,
                    user_id=self.current_user_id,
                    chat_id=chat_id
                )
            else:
                await asyncio.to_thread(
                    self.memory.manage,
                    user_message,
                    assistant_message,
                    self.current_user_id,
                    chat_id
                )
        except Exception as e:
            logger.warning(f"Async memory manage failed: {e}")

    async def _manage_memory_async_with_queue(self, user_message: str, response_queue: asyncio.Queue, history: List[Dict[str, str]]) -> None:
        """å¼‚æ­¥ç®¡ç†è®°å¿†åˆ°åå°ï¼ˆä½¿ç”¨é˜Ÿåˆ—è·å–å®Œæ•´å›å¤ï¼Œç¡®ä¿åœ¨æµå¼è¾“å‡ºç»“æŸåè°ƒç”¨ï¼‰ã€‚"""
        try:
            # ä»é˜Ÿåˆ—è·å–å®Œæ•´å›å¤
            assistant_message = await response_queue.get()
            chat_id = f"chat_{int(time.time())}"
            
            # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œæœªå®ç°æ—¶å›é€€åˆ°çº¿ç¨‹æ± å°è£…çš„åŒæ­¥æ¥å£
            if hasattr(self.memory, "manage_async"):
                await self.memory.manage_async(
                    user_text=user_message,
                    assistant_text=assistant_message,
                    user_id=self.current_user_id,
                    chat_id=chat_id
                )
            else:
                await asyncio.to_thread(
                    self.memory.manage,
                    user_message,
                    assistant_message,
                    self.current_user_id,
                    chat_id
                )
        except Exception as e:
            logger.warning(f"Async memory manage with queue failed: {e}")

    async def _add_to_memory_async(self, message: str, history: List[Dict[str, str]]) -> None:
        """å¼‚æ­¥æ·»åŠ è®°å¿†åˆ°åå°ï¼ˆä¸é˜»å¡ Gradio äº‹ä»¶å¾ªç¯ï¼‰ã€‚"""
        try:
            chat_id = f"chat_{int(time.time())}"
            
            # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ç”¨äºè®°å¿†æå–
            conversation_context = self._build_conversation_context(message, history)
            
            # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œæœªå®ç°æ—¶å›é€€åˆ°çº¿ç¨‹æ± å°è£…çš„åŒæ­¥æ¥å£
            if hasattr(self.memory, "add_async"):
                await self.memory.add_async(
                    text=conversation_context,
                    user_id=self.current_user_id,
                    chat_id=chat_id
                )
            else:
                await asyncio.to_thread(
                    self.memory.add,
                    conversation_context,
                    self.current_user_id,
                    chat_id
                )
        except Exception as e:
            logger.warning(f"Async memory add failed: {e}")

    
    def _build_conversation_context(self, message: str, history: List[Dict[str, str]]) -> str:
        """æ„å»ºç”¨äºè®°å¿†æå–çš„å¯¹è¯ä¸Šä¸‹æ–‡ã€‚"""
        # åŒ…å«æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆæœ€å¤š3è½®ï¼‰
        context_parts = []
        history_pairs = self._history_pairs(history)
        
        for user_msg, ai_msg in history_pairs[-3:]:
            context_parts.append(f"ç”¨æˆ·: {user_msg}")
            context_parts.append(f"åŠ©æ‰‹: {ai_msg}")
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        context_parts.append(f"ç”¨æˆ·: {message}")
        
        return "\n".join(context_parts)
    
    async def run_consolidation(self, progress=gr.Progress()) -> Tuple[str, str]:
        """Run memory consolidation with progress updates."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ", await asyncio.to_thread(self.get_all_memories)
        
        if self.is_consolidating:
            return "â³ å·©å›ºä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­...", await asyncio.to_thread(self.get_all_memories)
        
        self.is_consolidating = True
        self.consolidation_log = []
        
        try:
            self.consolidation_log.append(f"ğŸš€ å¼€å§‹å·©å›º - {datetime.now().strftime('%H:%M:%S')}")
            progress(0.1, desc="æ­£åœ¨æŸ¥è¯¢è®°å¿†...")
            
            # Run consolidation in a worker thread to keep UI responsive
            stats = await asyncio.to_thread(self.memory.consolidate, user_id=self.current_user_id)
            
            progress(0.9, desc="å·©å›ºå®Œæˆ")
            
            # Build result log
            log = []
            log.append(f"âœ… å·©å›ºå®Œæˆ - {datetime.now().strftime('%H:%M:%S')}")
            log.append(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            log.append(f"  - å¤„ç†è®°å¿†æ•°: {stats.memories_processed}")
            log.append(f"  - åˆ›å»ºè¯­ä¹‰æ•°: {stats.semantic_created}")
            
            self.consolidation_log.extend(log)
            progress(1.0, desc="å®Œæˆ")
            
            return "\n".join(self.consolidation_log), await asyncio.to_thread(self.get_all_memories)
            
        except Exception as e:
            error = f"âŒ å·©å›ºå¤±è´¥: {str(e)}"
            self.consolidation_log.append(error)
            return "\n".join(self.consolidation_log), await asyncio.to_thread(self.get_all_memories)
        finally:
            self.is_consolidating = False
    
    async def reset_memories(self) -> Tuple[str, str]:
        """Reset all memories for current user."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ", ""
        
        try:
            count = await asyncio.to_thread(self.memory.reset, self.current_user_id)
            self.chat_history = []
            return f"âœ… å·²åˆ é™¤ {count} æ¡è®°å¿†", await asyncio.to_thread(self.get_all_memories)
        except Exception as e:
            return f"âŒ é‡ç½®å¤±è´¥: {str(e)}", await asyncio.to_thread(self.get_all_memories)


def create_demo_interface():
    """Create and return the Gradio interface."""
    app = MemoryDemoApp()
    
    with gr.Blocks(title="AI Memory System Demo", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ§  AI Memory System å¯è§†åŒ–æµ‹è¯•")
        gr.Markdown("åŸºäºè®¤çŸ¥å¿ƒç†å­¦çš„AIé•¿æœŸè®°å¿†ç³»ç»Ÿæ¼”ç¤º")
        
        with gr.Row():
            user_id_input = gr.Textbox(label="ç”¨æˆ·ID", value="demo_user", scale=2)
            init_btn = gr.Button("ğŸ”„ åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary", scale=1)
            init_status = gr.Textbox(label="çŠ¶æ€", interactive=False, scale=2)
        
        with gr.Row():
            # Left panel - Memory display
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“š è®°å¿†åº“")
                memory_display = gr.Textbox(
                    label="å®æ—¶è®°å¿†çŠ¶æ€",
                    lines=25,
                    max_lines=30,
                    interactive=False
                )
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°è®°å¿†", variant="secondary")
                
                gr.Markdown("### âš™ï¸ è®°å¿†å·©å›º")
                consolidate_btn = gr.Button("ğŸ”§ è¿è¡Œå·©å›º", variant="primary")
                consolidation_output = gr.Textbox(label="å·©å›ºæ—¥å¿—", lines=8, interactive=False)
                
                reset_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºè®°å¿†", variant="stop")
                reset_output = gr.Textbox(label="æ“ä½œç»“æœ", lines=2, interactive=False)
            
            # Right panel - Chat interface
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ’¬ å¯¹è¯æµ‹è¯•")
                chatbot = gr.Chatbot(label="å¯¹è¯å†å²", height=400, type='messages')
                msg_input = gr.Textbox(label="è¾“å…¥æ¶ˆæ¯", placeholder="è¾“å…¥è¦è®°å¿†çš„å†…å®¹...")
                send_btn = gr.Button("å‘é€", variant="primary")
                
                gr.Markdown("### ğŸ’¡ æµ‹è¯•å»ºè®®")
                gr.Markdown("""
                - è¾“å…¥ä¸ªäººä¿¡æ¯: "æˆ‘æ˜¯åŒ—äº¬å¤§å­¦è®¡ç®—æœºä¸“ä¸šçš„å­¦ç”Ÿ"
                - æ˜ç¡®è®°å¿†è¯·æ±‚: "è¯·è®°ä½æˆ‘å–œæ¬¢å–å’–å•¡"
                - é¡¹ç›®ä¿¡æ¯: "æˆ‘æ­£åœ¨å¼€å‘ä¸€ä¸ªAIè®°å¿†ç³»ç»Ÿ"
                - é—²èŠæµ‹è¯•: "ä½ å¥½" (ä¸ä¼šè¢«è®°å½•)
                """)
        
        # Event handlers
        init_btn.click(
            fn=app.initialize_memory_system,
            inputs=[user_id_input],
            outputs=[init_status]
        ).then(
            fn=app.get_all_memories,
            outputs=[memory_display]
        )
        
        refresh_btn.click(fn=app.get_all_memories, outputs=[memory_display])
        
        send_btn.click(
            fn=app.chat_stream,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, memory_display]
        )
        
        msg_input.submit(
            fn=app.chat_stream,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, memory_display]
        )
        
        consolidate_btn.click(
            fn=app.run_consolidation,
            outputs=[consolidation_output, memory_display]
        )
        
        reset_btn.click(
            fn=app.reset_memories,
            outputs=[reset_output, memory_display]
        )
    
    return demo


if __name__ == "__main__":
    demo = create_demo_interface()
    demo.launch(server_name="0.0.0.0", server_port=7861, share=False)
