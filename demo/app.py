"""
Gradio-based visualization demo for AI Memory System.

Features:
- Left panel: Real-time memory display (episodic + semantic)
- Right panel: Chat interface
- Memory consolidation with progress display
- Scheduled consolidation support
"""

import gradio as gr
import asyncio
import time
import json
import threading
import logging
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import sys
import os
from langfuse import observe, get_client
# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.memory_system import Memory, MemoryConfig, MemoryRecord, ConsolidationStats


# Setup logger
logger = logging.getLogger(__name__)


class MemoryDemoApp:
    """Main demo application class."""
    
    def __init__(self):
        """Initialize the demo application."""
        self.memory: Optional[Memory] = None
        self.current_user_id: str = "demo_user"
        self.chat_history: List[Dict[str, str]] = []
        self.consolidation_log: List[str] = []
        self.is_consolidating: bool = False
        self.scheduled_task: Optional[threading.Timer] = None
        
    def initialize_memory_system(self, user_id: str) -> str:
        """Initialize or reinitialize the memory system."""
        try:
            if not user_id.strip():
                user_id = "demo_user"
            self.current_user_id = user_id.strip()
            
            config = MemoryConfig()
            config.collection_name = f"demo_memories_{self.current_user_id}"
            self.memory = Memory(config)
            self.chat_history = []
            self.consolidation_log = []
            
            return f"âœ… è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼ç”¨æˆ·ID: {self.current_user_id}"
        except Exception as e:
            return f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}"
    
    def get_all_memories(self) -> str:
        """Get all memories for current user and format as display text."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"
        
        try:
            # Query episodic memories
            episodic = self.memory._store.query(
                filter_expr=f'user_id == "{self.current_user_id}" and memory_type == "episodic"',
                output_fields=["id", "text", "ts", "group_id"],
                limit=100
            )
            
            # Query semantic memories
            semantic = self.memory._store.query(
                filter_expr=f'user_id == "{self.current_user_id}" and memory_type == "semantic"',
                output_fields=["id", "text", "ts"],
                limit=100
            )
            
            output = []
            output.append(f"ğŸ“Š è®°å¿†ç»Ÿè®¡ - ç”¨æˆ·: {self.current_user_id}")
            output.append(f"{'='*50}")
            output.append(f"æƒ…æ™¯è®°å¿†: {len(episodic)} æ¡ | è¯­ä¹‰è®°å¿†: {len(semantic)} æ¡")
            output.append("")
            
            # Display episodic memories
            output.append("ğŸ¬ æƒ…æ™¯è®°å¿† (Episodic)")
            output.append("-" * 40)
            if episodic:
                for mem in sorted(episodic, key=lambda x: x.get("ts", 0), reverse=True):
                    ts = mem.get("ts", 0)
                    time_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M") if ts else "N/A"
                    text = mem.get("text", "")
                    group_id = mem.get("group_id", -1)
                    group_info = f" [ç»„:{group_id}]" if group_id != -1 else " [æœªåˆ†ç»„]"
                    output.append(f"[ID:{mem.get('id')}] æ—¶é—´:{time_str}{group_info}")
                    output.append(f"  å†…å®¹: {text}")
                    output.append("")
            else:
                output.append("  (æš‚æ— æƒ…æ™¯è®°å¿†)")
                output.append("")
            
            # Display semantic memories
            output.append("ğŸ§  è¯­ä¹‰è®°å¿† (Semantic)")
            output.append("-" * 40)
            if semantic:
                for mem in sorted(semantic, key=lambda x: x.get("ts", 0), reverse=True):
                    ts = mem.get("ts", 0)
                    time_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M") if ts else "N/A"
                    fact = mem.get("text", "")
                    output.append(f"[ID:{mem.get('id')}] æ—¶é—´:{time_str}")
                    output.append(f"  å†…å®¹: {fact}")
                    output.append("")
            else:
                output.append("  (æš‚æ— è¯­ä¹‰è®°å¿†)")
            
            return "\n".join(output)
            
        except Exception as e:
            return f"âŒ è·å–è®°å¿†å¤±è´¥: {str(e)}"

    def get_narrative_groups(self) -> str:
        """Get all narrative groups for current user and format as display text."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"
        
        try:
            # Get groups collection name for current user
            groups_collection_name = f"groups_{self.current_user_id}"
            
            # Check if groups collection exists
            if not self.memory._store._client.has_collection(groups_collection_name):
                return f"ğŸ“‹ å™äº‹ç»„ - ç”¨æˆ·: {self.current_user_id}\n\n(æš‚æ— å™äº‹ç»„)"
            
            # Query all groups for the user
            groups = self.memory._store._client.query(
                collection_name=groups_collection_name,
                filter=f'user_id == "{self.current_user_id}"',
                output_fields=["group_id", "size", "centroid_vector"],
                limit=1000
            )
            
            output = []
            output.append(f"ğŸ“‹ å™äº‹ç»„ - ç”¨æˆ·: {self.current_user_id}")
            output.append(f"{'='*50}")
            output.append(f"å™äº‹ç»„æ€»æ•°: {len(groups)} ä¸ª")
            output.append("")
            
            if groups:
                # Sort groups by size (largest first)
                groups_sorted = sorted(groups, key=lambda x: x.get("size", 0), reverse=True)
                
                for group in groups_sorted:
                    group_id = group.get("group_id", 0)
                    size = group.get("size", 0)
                    
                    output.append(f"ğŸ”— å™äº‹ç»„ [ID:{group_id}]")
                    output.append(f"   æˆå‘˜æ•°é‡: {size}")
                    
                    # Get members of this group
                    try:
                        members = self.memory._store.query(
                            filter_expr=f'group_id == {group_id} and user_id == "{self.current_user_id}"',
                            output_fields=["id", "text", "ts"],
                            limit=1000
                        )
                        
                        if members:
                            output.append(f"   æˆå‘˜åˆ—è¡¨:")
                            for mem in sorted(members, key=lambda x: x.get("ts", 0), reverse=True):
                                ts = mem.get("ts", 0)
                                time_str = datetime.fromtimestamp(ts).strftime("%m-%d %H:%M") if ts else "N/A"
                                text = mem.get("text", "")
                                # Truncate long text
                                if len(text) > 50:
                                    text = text[:47] + "..."
                                output.append(f"     [ID:{mem.get('id')}] {time_str} - {text}")
                        
                    except Exception as e:
                        output.append(f"   (è·å–æˆå‘˜å¤±è´¥: {str(e)})")
                    
                    output.append("")
            else:
                output.append("(æš‚æ— å™äº‹ç»„)")
                output.append("")
            
            # Add statistics
            try:
                # Count ungrouped episodic memories
                ungrouped = self.memory._store.query(
                    filter_expr=f'user_id == "{self.current_user_id}" and memory_type == "episodic" and group_id == -1',
                    output_fields=["id"],
                    limit=10000
                )
                
                total_episodic = self.memory._store.query(
                    filter_expr=f'user_id == "{self.current_user_id}" and memory_type == "episodic"',
                    output_fields=["id"],
                    limit=10000
                )
                
                output.append("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
                output.append(f"   æ€»æƒ…æ™¯è®°å¿†: {len(total_episodic)} æ¡")
                output.append(f"   å·²åˆ†ç»„è®°å¿†: {len(total_episodic) - len(ungrouped)} æ¡")
                output.append(f"   æœªåˆ†ç»„è®°å¿†: {len(ungrouped)} æ¡")
                if len(total_episodic) > 0:
                    grouped_ratio = (len(total_episodic) - len(ungrouped)) / len(total_episodic) * 100
                    output.append(f"   åˆ†ç»„æ¯”ä¾‹: {grouped_ratio:.1f}%")
                
            except Exception as e:
                output.append(f"   (ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {str(e)})")
            
            return "\n".join(output)
            
        except Exception as e:
            return f"âŒ è·å–å™äº‹ç»„å¤±è´¥: {str(e)}"

    @observe(as_type="agent") 
    async def chat(self, message: str, history: List[Any]) -> Tuple[str, List[Dict[str, str]], str]:
        """Process chat message with intelligent reconsolidation: search â†’ respond â†’ judge usage â†’ reconsolidate used memories."""
        history_messages = self._normalize_history(history)
    
        get_client().update_current_trace(
            session_id=f"demo_chat_{self.current_user_id}_{int(time.time())}",
            user_id=self.current_user_id,
            tags=["demo_chat", "memory_system"],
            metadata={
                "app": "MemoryDemoApp",
                "message_length": len(message),
                "history_length": len(history_messages)
            }
        )
        
        if not self.memory:
            return "", history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"}
            ], await asyncio.to_thread(self.get_all_memories)
        
        if not message.strip():
            return "", history_messages, await asyncio.to_thread(self.get_all_memories)
        
        try:
            # 1. å‡†å¤‡æ¶ˆæ¯å’Œä¸Šä¸‹æ–‡
            prepared_messages = self._prepare_messages(message, history_messages)
            
            # 2. æ£€ç´¢ç›¸å…³è®°å¿†
            relevant_memories = await asyncio.to_thread(
                self.memory.search,
                message,
                self.current_user_id
            )
            
            # 3. æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆä¼ å…¥ historyï¼‰
            full_context = self._build_context_with_memories(message, relevant_memories, history_messages)
            
            # 4. è°ƒç”¨LLMç”Ÿæˆå›å¤ï¼ˆæ”¾åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰
            ai_response = await asyncio.to_thread(self._generate_response, full_context, prepared_messages)
            
            # 5. è®°å¿†ç®¡ç†
            asyncio.create_task(self._manage_memory_async(message, ai_response, history_messages))
            
            # æ„å»ºæœ€ç»ˆå“åº”
            final_response = ai_response
            new_history = history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": final_response}
            ]
            return "", new_history, await asyncio.to_thread(self.get_all_memories)
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            return "", history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": error_msg}
            ], await asyncio.to_thread(self.get_all_memories)
    
    async def chat_stream(self, message: str, history: List[Any]):
        """Process chat message with streaming response and intelligent reconsolidation."""
        history_messages = self._normalize_history(history)
        
        if not self.memory:
            error_response = "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"
            new_history = history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": error_response}
            ]
            yield new_history, await asyncio.to_thread(self.get_all_memories), await asyncio.to_thread(self.get_narrative_groups)
            return
        
        if not message.strip():
            yield history_messages, await asyncio.to_thread(self.get_all_memories), await asyncio.to_thread(self.get_narrative_groups)
            return
        
        try:
            # 1. å‡†å¤‡æ¶ˆæ¯å’Œä¸Šä¸‹æ–‡
            prepared_messages = self._prepare_messages(message, history_messages)
            
            # 2. æ£€ç´¢ç›¸å…³è®°å¿†
            relevant_memories = await asyncio.to_thread(
                self.memory.search,
                message,
                self.current_user_id
            )
            
            # 3. æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆä¼ å…¥ historyï¼‰
            full_context = self._build_context_with_memories(message, relevant_memories, history_messages)
            
            # 4. åˆ›å»ºç”¨äºæ”¶é›†å®Œæ•´å›å¤çš„é˜Ÿåˆ—
            response_queue = asyncio.Queue()
            
            # 5. å¯åŠ¨æµå¼å“åº”ç”Ÿæˆ
            accumulated_response = ""
            new_history = history_messages + [{"role": "user", "content": message}]
            
            # å…ˆæ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            yield new_history, await asyncio.to_thread(self.get_all_memories), await asyncio.to_thread(self.get_narrative_groups)
            
            # æµå¼ç”Ÿæˆå›å¤
            async for chunk in self._generate_response_stream(full_context, prepared_messages):
                accumulated_response += chunk
                current_history = new_history + [{"role": "assistant", "content": accumulated_response}]
                yield current_history, await asyncio.to_thread(self.get_all_memories), await asyncio.to_thread(self.get_narrative_groups)
            
            # 6. å°†å®Œæ•´å›å¤æ”¾å…¥é˜Ÿåˆ—ä¾›è®°å¿†å¤„ç†ä½¿ç”¨
            await response_queue.put(accumulated_response)
            
            # 6. å¯åŠ¨è®°å¿†å¤„ç†ä»»åŠ¡ï¼ˆåœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼‰
            asyncio.create_task(self._process_memory_async(
                user_message=message,
                response_queue=response_queue,
                history_messages=history_messages,
                relevant_memories=relevant_memories,
                full_context=full_context
            ))
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            error_history = history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": error_msg}
            ]
            yield error_history, await asyncio.to_thread(self.get_all_memories), await asyncio.to_thread(self.get_narrative_groups)
    
    def _normalize_history(self, history: List[Any]) -> List[Dict[str, str]]:
        """Normalize Chatbot history to the messages format Gradio expects."""
        normalized: List[Dict[str, str]] = []
        
        for item in history or []:
            if isinstance(item, dict) and "role" in item and "content" in item:
                normalized.append({"role": str(item["role"]), "content": str(item["content"])})
            elif hasattr(item, "role") and hasattr(item, "content"):
                role = getattr(item, "role", None)
                content = getattr(item, "content", None)
                if role is not None and content is not None:
                    normalized.append({"role": str(role), "content": str(content)})
            elif isinstance(item, (list, tuple)) and len(item) == 2:
                user_msg, ai_msg = item
                normalized.append({"role": "user", "content": str(user_msg)})
                normalized.append({"role": "assistant", "content": str(ai_msg)})
        
        return normalized
    
    def _history_pairs(self, history: List[Dict[str, str]]) -> List[Tuple[str, str]]:
        """Convert message-style history into user/assistant pairs for logging or prompts."""
        pairs: List[Tuple[str, str]] = []
        last_user: Optional[str] = None
        
        for msg in history:
            if msg.get("role") == "user":
                last_user = msg.get("content", "")
            elif msg.get("role") == "assistant" and last_user is not None:
                pairs.append((last_user, msg.get("content", "")))
                last_user = None
        
        return pairs
    
    def _prepare_messages(self, message: str, history: List[Dict[str, str]]) -> List[Dict]:
        """å‡†å¤‡å’Œæ ‡å‡†åŒ–æ¶ˆæ¯ï¼ŒåŒ…å«å†å²å¯¹è¯ä¸Šä¸‹æ–‡ã€‚"""
        messages = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in history[-50:]
            if isinstance(msg, dict) and "role" in msg and "content" in msg
        ]
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        messages.append({"role": "user", "content": message})
        
        return messages
    
    
    def _build_context_with_memories(self, message: str, memories: Dict[str, List[MemoryRecord]], history: List[Dict[str, str]]) -> str:
        """æ„å»ºåŒ…å«è®°å¿†çš„å®Œæ•´ä¸Šä¸‹æ–‡ã€‚"""
        context_parts = []
        history_pairs = self._history_pairs(history)
        
        # 1. æƒ…æ™¯è®°å¿†éƒ¨åˆ†
        context_parts.append("Here are the episodic memories:")
        episodic_memories = memories.get("episodic", [])
        if episodic_memories:
            for i, mem in enumerate(episodic_memories[:3], 1):
                context_parts.append(f"{i}. {mem.text}")
        else:
            context_parts.append("(No episodic memories)")
        context_parts.append("")
        
        # 2. è¯­ä¹‰è®°å¿†éƒ¨åˆ†
        context_parts.append("Here are the semantic memories:")
        semantic_memories = memories.get("semantic", [])
        if semantic_memories:
            for i, mem in enumerate(semantic_memories[:3], 1):
                context_parts.append(f"{i}. {mem.text}")
        else:
            context_parts.append("(No semantic memories)")
        context_parts.append("")
        
        # 3. å†å²å¯¹è¯éƒ¨åˆ†
        context_parts.append("Here are the history messages:")
        if history_pairs:
            for i, (user_msg, ai_msg) in enumerate(history_pairs[-3:], 1):
                context_parts.append(f"Turn {i}:")
                context_parts.append(f"  User: {user_msg}")
                context_parts.append(f"  Assistant: {ai_msg}")
        else:
            context_parts.append("(No history messages)")
        context_parts.append("")
        
        # 4. å½“å‰ä»»åŠ¡
        context_parts.append("Here are the task:")
        context_parts.append(message)
        
        return "\n".join(context_parts)
    
    def _generate_response(self, context: str, messages: List[Dict]) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆå›å¤ã€‚"""
        # å¯¼å…¥ MEMORY_ANSWER_PROMPT
        try:
            from prompts import MEMORY_ANSWER_PROMPT
            system_prompt = f"{MEMORY_ANSWER_PROMPT}\n\nUser ID: {self.current_user_id}\n\n{context}"
        except ImportError:
            system_prompt = f"""You are an AI assistant with long-term memory capabilities. User ID: {self.current_user_id}
Please answer based on the user's messages and relevant memories. If there are relevant memories, reflect that you remember the user's information in your response.
Maintain a friendly and natural conversation style.

{context}"""
        
        try:
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = messages[-1]["content"] if messages else ""
            ai_response = self.memory._llm_client.chat(system_prompt, user_message)
            return ai_response
        except Exception as llm_error:
            return f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤ã€‚é”™è¯¯: {str(llm_error)}"
    
    async def _generate_response_stream(self, context: str, messages: List[Dict]):
        """ä½¿ç”¨LLMæµå¼ç”Ÿæˆå›å¤ã€‚"""
        # å¯¼å…¥ MEMORY_ANSWER_PROMPT
        try:
            from prompts import MEMORY_ANSWER_PROMPT
            system_prompt = f"{MEMORY_ANSWER_PROMPT}\n\n{context}"
        except ImportError:
            system_prompt = f"""You are an AI assistant with long-term memory capabilities. User ID: {self.current_user_id}
Please answer based on the user's messages and relevant memories. If there are relevant memories, reflect that you remember the user's information in your response.
Maintain a friendly and natural conversation style.

{context}"""
        
        try:
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = messages[-1]["content"] if messages else ""
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæµå¼è°ƒç”¨
            response_stream = await asyncio.to_thread(
                self.memory._llm_client.chat_stream, system_prompt, user_message
            )
            
            accumulated_response = ""
            for chunk in response_stream:
                accumulated_response += chunk
                yield chunk
                
        except Exception as llm_error:
            yield f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤ã€‚é”™è¯¯: {str(llm_error)}"
    
    async def _manage_memory_async(self, user_message: str, assistant_message: str, history: List[Dict[str, str]]) -> None:
        """å¼‚æ­¥ç®¡ç†è®°å¿†åˆ°åå°ï¼ˆä¸é˜»å¡ Gradio äº‹ä»¶å¾ªç¯ï¼‰ã€‚"""
        try:
            chat_id = f"chat_{int(time.time())}"
            
            # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œæœªå®ç°æ—¶å›é€€åˆ°çº¿ç¨‹æ± å°è£…çš„åŒæ­¥æ¥å£
            if hasattr(self.memory, "manage_async"):
                await self.memory.manage_async(
                    user_text=user_message,
                    assistant_text=assistant_message,
                    user_id=self.current_user_id,
                    chat_id=chat_id
                )
            else:
                await asyncio.to_thread(
                    self.memory.manage,
                    user_message,
                    assistant_message,
                    self.current_user_id,
                    chat_id
                )
        except Exception as e:
            logger.warning(f"Async memory manage failed: {e}")

    async def _manage_memory_async_with_queue(self, user_message: str, response_queue: asyncio.Queue, history: List[Dict[str, str]]) -> None:
        """å¼‚æ­¥ç®¡ç†è®°å¿†åˆ°åå°ï¼ˆä½¿ç”¨é˜Ÿåˆ—è·å–å®Œæ•´å›å¤ï¼Œç¡®ä¿åœ¨æµå¼è¾“å‡ºç»“æŸåè°ƒç”¨ï¼‰ã€‚"""
        try:
            # ä»é˜Ÿåˆ—è·å–å®Œæ•´å›å¤
            assistant_message = await response_queue.get()
            chat_id = f"chat_{int(time.time())}"
            
            # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œæœªå®ç°æ—¶å›é€€åˆ°çº¿ç¨‹æ± å°è£…çš„åŒæ­¥æ¥å£
            if hasattr(self.memory, "manage_async"):
                await self.memory.manage_async(
                    user_text=user_message,
                    assistant_text=assistant_message,
                    user_id=self.current_user_id,
                    chat_id=chat_id
                )
            else:
                await asyncio.to_thread(
                    self.memory.manage,
                    user_message,
                    assistant_message,
                    self.current_user_id,
                    chat_id
                )
        except Exception as e:
            logger.warning(f"Async memory manage with queue failed: {e}")

    async def _process_memory_async(
        self,
        user_message: str,
        response_queue: asyncio.Queue,
        history_messages: List[Dict[str, str]],
        relevant_memories: Dict[str, List[MemoryRecord]],
        full_context: str
    ) -> None:
        """å¼‚æ­¥å¤„ç†è®°å¿†ï¼šåˆ¤æ–­ä½¿ç”¨ â†’ å™äº‹åˆ†ç»„ â†’ manage"""
        try:
            # 1. ä»é˜Ÿåˆ—è·å–å®Œæ•´å›å¤
            assistant_message = await response_queue.get()
            
            # 2. è°ƒç”¨ MemoryUsageJudge åˆ¤æ–­å“ªäº›æƒ…æ™¯è®°å¿†è¢«ä½¿ç”¨
            episodic_texts = [mem.text for mem in relevant_memories.get("episodic", [])]
            semantic_texts = [mem.text for mem in relevant_memories.get("semantic", [])]
            
            used_episodic_texts = await asyncio.to_thread(
                self.memory._memory_usage_judge.judge_used_memories,
                system_prompt=full_context,
                episodic_memories=episodic_texts,
                semantic_memories=semantic_texts,
                message_history=history_messages,
                final_reply=assistant_message
            )
            
            # 3. æ‰¾å‡ºè¢«ä½¿ç”¨çš„æƒ…æ™¯è®°å¿†çš„ ID
            used_memory_ids = []
            for mem in relevant_memories.get("episodic", []):
                if mem.text in used_episodic_texts:
                    used_memory_ids.append(mem.id)
            
            # 4. å¯¹è¢«ä½¿ç”¨çš„æƒ…æ™¯è®°å¿†æ‰§è¡Œå™äº‹åˆ†ç»„
            if used_memory_ids:
                await asyncio.to_thread(
                    self.memory.assign_to_narrative_group,
                    memory_ids=used_memory_ids,
                    user_id=self.current_user_id
                )
                logger.info(f"Assigned {len(used_memory_ids)} episodic memories to narrative groups")
            
            # 5. æ‰§è¡Œ manage ç®¡ç†è®°å¿†
            chat_id = f"chat_{int(time.time())}"
            if hasattr(self.memory, "manage_async"):
                await self.memory.manage_async(
                    user_text=user_message,
                    assistant_text=assistant_message,
                    user_id=self.current_user_id,
                    chat_id=chat_id
                )
            else:
                await asyncio.to_thread(
                    self.memory.manage,
                    user_message,
                    assistant_message,
                    self.current_user_id,
                    chat_id
                )
        except Exception as e:
            logger.warning(f"Async memory processing failed: {e}")

    async def _add_to_memory_async(self, message: str, history: List[Dict[str, str]]) -> None:
        """å¼‚æ­¥æ·»åŠ è®°å¿†åˆ°åå°ï¼ˆä¸é˜»å¡ Gradio äº‹ä»¶å¾ªç¯ï¼‰ã€‚"""
        try:
            chat_id = f"chat_{int(time.time())}"
            
            # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ç”¨äºè®°å¿†æå–
            conversation_context = self._build_conversation_context(message, history)
            
            # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œæœªå®ç°æ—¶å›é€€åˆ°çº¿ç¨‹æ± å°è£…çš„åŒæ­¥æ¥å£
            if hasattr(self.memory, "add_async"):
                await self.memory.add_async(
                    text=conversation_context,
                    user_id=self.current_user_id,
                    chat_id=chat_id
                )
            else:
                await asyncio.to_thread(
                    self.memory.add,
                    conversation_context,
                    self.current_user_id,
                    chat_id
                )
        except Exception as e:
            logger.warning(f"Async memory add failed: {e}")

    
    def _build_conversation_context(self, message: str, history: List[Dict[str, str]]) -> str:
        """æ„å»ºç”¨äºè®°å¿†æå–çš„å¯¹è¯ä¸Šä¸‹æ–‡ã€‚"""
        # åŒ…å«æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆæœ€å¤š3è½®ï¼‰
        context_parts = []
        history_pairs = self._history_pairs(history)
        
        for user_msg, ai_msg in history_pairs[-3:]:
            context_parts.append(f"ç”¨æˆ·: {user_msg}")
            context_parts.append(f"åŠ©æ‰‹: {ai_msg}")
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        context_parts.append(f"ç”¨æˆ·: {message}")
        
        return "\n".join(context_parts)
    
    async def run_consolidation(self, progress=gr.Progress()) -> Tuple[str, str]:
        """Run memory consolidation with progress updates."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ", await asyncio.to_thread(self.get_all_memories)
        
        if self.is_consolidating:
            return "â³ å·©å›ºä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­...", await asyncio.to_thread(self.get_all_memories)
        
        self.is_consolidating = True
        self.consolidation_log = []
        
        try:
            self.consolidation_log.append(f"ğŸš€ å¼€å§‹å·©å›º - {datetime.now().strftime('%H:%M:%S')}")
            progress(0.1, desc="æ­£åœ¨æŸ¥è¯¢è®°å¿†...")
            
            # Run consolidation in a worker thread to keep UI responsive
            stats = await asyncio.to_thread(self.memory.consolidate, user_id=self.current_user_id)
            
            progress(0.9, desc="å·©å›ºå®Œæˆ")
            
            # Build result log
            log = []
            log.append(f"âœ… å·©å›ºå®Œæˆ - {datetime.now().strftime('%H:%M:%S')}")
            log.append(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            log.append(f"  - å¤„ç†è®°å¿†æ•°: {stats.memories_processed}")
            log.append(f"  - åˆ›å»ºè¯­ä¹‰æ•°: {stats.semantic_created}")
            
            self.consolidation_log.extend(log)
            progress(1.0, desc="å®Œæˆ")
            
            return "\n".join(self.consolidation_log), await asyncio.to_thread(self.get_all_memories)
            
        except Exception as e:
            error = f"âŒ å·©å›ºå¤±è´¥: {str(e)}"
            self.consolidation_log.append(error)
            return "\n".join(self.consolidation_log), await asyncio.to_thread(self.get_all_memories)
        finally:
            self.is_consolidating = False
    
    async def reset_memories(self) -> Tuple[str, str]:
        """Reset all memories for current user."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ", ""
        
        try:
            count = await asyncio.to_thread(self.memory.reset, self.current_user_id)
            self.chat_history = []
            return f"âœ… å·²åˆ é™¤ {count} æ¡è®°å¿†", await asyncio.to_thread(self.get_all_memories)
        except Exception as e:
            return f"âŒ é‡ç½®å¤±è´¥: {str(e)}", await asyncio.to_thread(self.get_all_memories)


def create_demo_interface():
    """Create and return the Gradio interface."""
    app = MemoryDemoApp()
    
    with gr.Blocks(title="AI Memory System Demo", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ§  AI Memory System å¯è§†åŒ–æµ‹è¯•")
        gr.Markdown("åŸºäºè®¤çŸ¥å¿ƒç†å­¦çš„AIé•¿æœŸè®°å¿†ç³»ç»Ÿæ¼”ç¤º")
        
        with gr.Row():
            user_id_input = gr.Textbox(label="ç”¨æˆ·ID", value="demo_user", scale=2)
            init_btn = gr.Button("ğŸ”„ åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary", scale=1)
            init_status = gr.Textbox(label="çŠ¶æ€", interactive=False, scale=2)
        
        with gr.Row():
            # Left panel - Memory display
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“š è®°å¿†åº“")
                memory_display = gr.Textbox(
                    label="å®æ—¶è®°å¿†çŠ¶æ€",
                    lines=20,
                    max_lines=25,
                    interactive=False
                )
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°è®°å¿†", variant="secondary")
                
                gr.Markdown("### âš™ï¸ è®°å¿†å·©å›º")
                consolidate_btn = gr.Button("ğŸ”§ è¿è¡Œå·©å›º", variant="primary")
                consolidation_output = gr.Textbox(label="å·©å›ºæ—¥å¿—", lines=6, interactive=False)
                
                reset_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºè®°å¿†", variant="stop")
                reset_output = gr.Textbox(label="æ“ä½œç»“æœ", lines=2, interactive=False)
            
            # Middle panel - Chat interface
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ’¬ å¯¹è¯æµ‹è¯•")
                chatbot = gr.Chatbot(label="å¯¹è¯å†å²", height=400, type='messages')
                msg_input = gr.Textbox(label="è¾“å…¥æ¶ˆæ¯", placeholder="è¾“å…¥è¦è®°å¿†çš„å†…å®¹...")
                send_btn = gr.Button("å‘é€", variant="primary")
                
                gr.Markdown("### ğŸ’¡ æµ‹è¯•å»ºè®®")
                gr.Markdown("""
                - è¾“å…¥ä¸ªäººä¿¡æ¯: "æˆ‘æ˜¯åŒ—äº¬å¤§å­¦è®¡ç®—æœºä¸“ä¸šçš„å­¦ç”Ÿ"
                - æ˜ç¡®è®°å¿†è¯·æ±‚: "è¯·è®°ä½æˆ‘å–œæ¬¢å–å’–å•¡"
                - é¡¹ç›®ä¿¡æ¯: "æˆ‘æ­£åœ¨å¼€å‘ä¸€ä¸ªAIè®°å¿†ç³»ç»Ÿ"
                - é—²èŠæµ‹è¯•: "ä½ å¥½" (ä¸ä¼šè¢«è®°å½•)
                """)
            
            # Right panel - Narrative groups display
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“‹ å™äº‹ç»„")
                groups_display = gr.Textbox(
                    label="å™äº‹ç»„çŠ¶æ€",
                    lines=25,
                    max_lines=30,
                    interactive=False
                )
                refresh_groups_btn = gr.Button("ğŸ”„ åˆ·æ–°å™äº‹ç»„", variant="secondary")
                
                gr.Markdown("### ğŸ“Š å™äº‹ç»Ÿè®¡")
                gr.Markdown("""
                **å™äº‹è®°å¿†åŠŸèƒ½è¯´æ˜:**
                - ğŸ”— å™äº‹ç»„å°†ç›¸å…³çš„æƒ…æ™¯è®°å¿†ç»„ç»‡åœ¨ä¸€èµ·
                - ğŸ“ˆ æ˜¾ç¤ºåˆ†ç»„ç»Ÿè®¡å’Œæˆå‘˜ä¿¡æ¯
                - ğŸ¯ åªæœ‰è¢«å®é™…ä½¿ç”¨çš„è®°å¿†æ‰ä¼šåˆ†ç»„
                - ğŸ”„ è‡ªåŠ¨ç»´æŠ¤ç»„ä¸­å¿ƒå‘é‡
                """)
        
        # Event handlers
        init_btn.click(
            fn=app.initialize_memory_system,
            inputs=[user_id_input],
            outputs=[init_status]
        ).then(
            fn=app.get_all_memories,
            outputs=[memory_display]
        ).then(
            fn=app.get_narrative_groups,
            outputs=[groups_display]
        )
        
        refresh_btn.click(fn=app.get_all_memories, outputs=[memory_display])
        
        refresh_groups_btn.click(fn=app.get_narrative_groups, outputs=[groups_display])
        
        send_btn.click(
            fn=app.chat_stream,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, memory_display, groups_display]
        )
        
        msg_input.submit(
            fn=app.chat_stream,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, memory_display, groups_display]
        )
        
        consolidate_btn.click(
            fn=app.run_consolidation,
            outputs=[consolidation_output, memory_display]
        )
        
        reset_btn.click(
            fn=app.reset_memories,
            outputs=[reset_output, memory_display]
        )
    
    return demo


if __name__ == "__main__":
    demo = create_demo_interface()
    demo.launch(server_name="0.0.0.0", server_port=7861, share=False)
