"""
Gradio-based visualization demo for AI Memory System.

Features:
- Left panel: Real-time memory display (episodic + semantic)
- Right panel: Chat interface
- Memory consolidation with progress display
- Scheduled consolidation support
"""

import gradio as gr
import asyncio
import time
import json
import threading
import logging
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import sys
import os

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.memory_system import Memory, MemoryConfig, MemoryRecord, ConsolidationStats

# Setup logger
logger = logging.getLogger(__name__)


class MemoryDemoApp:
    """Main demo application class."""
    
    def __init__(self):
        """Initialize the demo application."""
        self.memory: Optional[Memory] = None
        self.current_user_id: str = "demo_user"
        self.chat_history: List[Dict[str, str]] = []
        self.consolidation_log: List[str] = []
        self.is_consolidating: bool = False
        self.scheduled_task: Optional[threading.Timer] = None
        
    def initialize_memory_system(self, user_id: str) -> str:
        """Initialize or reinitialize the memory system."""
        try:
            if not user_id.strip():
                user_id = "demo_user"
            self.current_user_id = user_id.strip()
            
            config = MemoryConfig()
            config.collection_name = f"demo_memories_{self.current_user_id}"
            self.memory = Memory(config)
            self.chat_history = []
            self.consolidation_log = []
            
            return f"âœ… è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼ç”¨æˆ·ID: {self.current_user_id}"
        except Exception as e:
            return f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}"
    
    def get_all_memories(self) -> str:
        """Get all memories for current user and format as display text."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"
        
        try:
            # Query episodic memories
            episodic = self.memory._store.query(
                filter_expr=f'user_id == "{self.current_user_id}" and memory_type == "episodic"',
                output_fields=["id", "text", "hit_count", "ts", "metadata"],
                limit=100
            )
            
            # Query semantic memories
            semantic = self.memory._store.query(
                filter_expr=f'user_id == "{self.current_user_id}" and memory_type == "semantic"',
                output_fields=["id", "text", "hit_count", "ts", "metadata"],
                limit=100
            )
            
            output = []
            output.append(f"ğŸ“Š è®°å¿†ç»Ÿè®¡ - ç”¨æˆ·: {self.current_user_id}")
            output.append(f"{'='*50}")
            output.append(f"æƒ…æ™¯è®°å¿†: {len(episodic)} æ¡ | è¯­ä¹‰è®°å¿†: {len(semantic)} æ¡")
            output.append("")
            
            # Display episodic memories
            output.append("ğŸ¬ æƒ…æ™¯è®°å¿† (Episodic)")
            output.append("-" * 40)
            if episodic:
                for mem in sorted(episodic, key=lambda x: x.get("ts", 0), reverse=True):
                    ts = mem.get("ts", 0)
                    time_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M") if ts else "N/A"
                    hit = mem.get("hit_count", 0)
                    text = mem.get("text", "")
                    metadata = mem.get("metadata", {})
                    context = metadata.get("context", "")
                    output.append(f"[ID:{mem.get('id')}] æ—¶é—´:{time_str}")
                    output.append(f"  å†…å®¹: {text}")
                    if context:
                        output.append(f"  ä¸Šä¸‹æ–‡: {context}")
                    output.append("")
            else:
                output.append("  (æš‚æ— æƒ…æ™¯è®°å¿†)")
                output.append("")
            
            # Display semantic memories
            output.append("ğŸ§  è¯­ä¹‰è®°å¿† (Semantic)")
            output.append("-" * 40)
            if semantic:
                for mem in sorted(semantic, key=lambda x: x.get("ts", 0), reverse=True):
                    ts = mem.get("ts", 0)
                    time_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M") if ts else "N/A"
                    hit = mem.get("hit_count", 0)
                    metadata = mem.get("metadata", {})
                    fact = metadata.get("fact", mem.get("text", ""))
                    output.append(f"[ID:{mem.get('id')}] æ—¶é—´:{time_str}")
                    output.append(f"  å†…å®¹: {fact}")
                    output.append("")
            else:
                output.append("  (æš‚æ— è¯­ä¹‰è®°å¿†)")
            
            return "\n".join(output)
            
        except Exception as e:
            return f"âŒ è·å–è®°å¿†å¤±è´¥: {str(e)}"

    async def chat(self, message: str, history: List[Any]) -> Tuple[str, List[Dict[str, str]], str]:
        """Process chat message with intelligent reconsolidation: search â†’ respond â†’ judge usage â†’ reconsolidate used memories."""
        history_messages = self._normalize_history(history)
        
        if not self.memory:
            return "", history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"}
            ], await asyncio.to_thread(self.get_all_memories)
        
        if not message.strip():
            return "", history_messages, await asyncio.to_thread(self.get_all_memories)
        
        try:
            # 1. å‡†å¤‡æ¶ˆæ¯å’Œä¸Šä¸‹æ–‡
            prepared_messages = self._prepare_messages(message, history_messages)
            
            # 2. æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆç¦ç”¨åŒæ­¥é‡å·©å›ºï¼Œé¿å…é˜»å¡ï¼‰
            relevant_memories = await asyncio.to_thread(
                self.memory.search,
                message,
                self.current_user_id,
                5,
                False  # reconsolidate off here; we handle intelligently later
            )
            
            # 3. æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆä¼ å…¥ historyï¼‰
            full_context = self._build_context_with_memories(message, relevant_memories, history_messages)
            
            # 4. è°ƒç”¨LLMç”Ÿæˆå›å¤ï¼ˆæ”¾åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰
            ai_response = await asyncio.to_thread(self._generate_response, full_context, prepared_messages)
            
            # 5. æ™ºèƒ½å·©å›ºä¸å†™å…¥ï¼šåŸºäºå®é™…ä½¿ç”¨æƒ…å†µ
            asyncio.create_task(self._intelligent_reconsolidate_async(
                message, 
                relevant_memories, 
                full_context, 
                ai_response, 
                history_messages
            ))
            asyncio.create_task(self._add_to_memory_async(message, history_messages))
            
            # æ„å»ºæœ€ç»ˆå“åº”
            final_response = ai_response
            new_history = history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": final_response}
            ]
            return "", new_history, await asyncio.to_thread(self.get_all_memories)
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            return "", history_messages + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": error_msg}
            ], await asyncio.to_thread(self.get_all_memories)
    
    def _normalize_history(self, history: List[Any]) -> List[Dict[str, str]]:
        """Normalize Chatbot history to the messages format Gradio expects."""
        normalized: List[Dict[str, str]] = []
        
        for item in history or []:
            if isinstance(item, dict) and "role" in item and "content" in item:
                normalized.append({"role": str(item["role"]), "content": str(item["content"])})
            elif hasattr(item, "role") and hasattr(item, "content"):
                role = getattr(item, "role", None)
                content = getattr(item, "content", None)
                if role is not None and content is not None:
                    normalized.append({"role": str(role), "content": str(content)})
            elif isinstance(item, (list, tuple)) and len(item) == 2:
                user_msg, ai_msg = item
                normalized.append({"role": "user", "content": str(user_msg)})
                normalized.append({"role": "assistant", "content": str(ai_msg)})
        
        return normalized
    
    def _history_pairs(self, history: List[Dict[str, str]]) -> List[Tuple[str, str]]:
        """Convert message-style history into user/assistant pairs for logging or prompts."""
        pairs: List[Tuple[str, str]] = []
        last_user: Optional[str] = None
        
        for msg in history:
            if msg.get("role") == "user":
                last_user = msg.get("content", "")
            elif msg.get("role") == "assistant" and last_user is not None:
                pairs.append((last_user, msg.get("content", "")))
                last_user = None
        
        return pairs
    
    def _prepare_messages(self, message: str, history: List[Dict[str, str]]) -> List[Dict]:
        """å‡†å¤‡å’Œæ ‡å‡†åŒ–æ¶ˆæ¯ï¼ŒåŒ…å«å†å²å¯¹è¯ä¸Šä¸‹æ–‡ã€‚"""
        messages = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in history[-50:]
            if isinstance(msg, dict) and "role" in msg and "content" in msg
        ]
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        messages.append({"role": "user", "content": message})
        
        return messages
    
    def _fetch_relevant_memories(self, query: str) -> List[MemoryRecord]:
        """æ£€ç´¢ç›¸å…³è®°å¿†ã€‚"""
        try:
            results = self.memory.search(
                query=query,
                user_id=self.current_user_id,
                limit=5,
                reconsolidate=True
            )
            return results
        except Exception as e:
            logger.warning(f"Failed to fetch memories: {e}")
            return []
    
    def _build_context_with_memories(self, message: str, memories: List[MemoryRecord], history: List[Dict[str, str]]) -> str:
        """æ„å»ºåŒ…å«è®°å¿†çš„å®Œæ•´ä¸Šä¸‹æ–‡ã€‚"""
        context_parts = []
        
        # åˆ†ç¦»æƒ…æ™¯è®°å¿†å’Œè¯­ä¹‰è®°å¿†
        episodic_memories = [mem for mem in memories if mem.memory_type == "episodic"]
        semantic_memories = [mem for mem in memories if mem.memory_type == "semantic"]
        history_pairs = self._history_pairs(history)
        
        # 1. æƒ…æ™¯è®°å¿†éƒ¨åˆ†
        context_parts.append("Here are the episodic memories:")
        if episodic_memories:
            for i, mem in enumerate(episodic_memories[:3], 1):
                context_parts.append(f"{i}. {mem.text}")
        else:
            context_parts.append("(No episodic memories)")
        context_parts.append("")
        
        # 2. è¯­ä¹‰è®°å¿†éƒ¨åˆ†
        context_parts.append("Here are the semantic memories:")
        if semantic_memories:
            for i, mem in enumerate(semantic_memories[:3], 1):
                context_parts.append(f"{i}. {mem.text}")
        else:
            context_parts.append("(No semantic memories)")
        context_parts.append("")
        
        # 3. å†å²å¯¹è¯éƒ¨åˆ†
        context_parts.append("Here are the history messages:")
        if history_pairs:
            for i, (user_msg, ai_msg) in enumerate(history_pairs[-3:], 1):
                context_parts.append(f"Turn {i}:")
                context_parts.append(f"  User: {user_msg}")
                context_parts.append(f"  Assistant: {ai_msg}")
        else:
            context_parts.append("(No history messages)")
        context_parts.append("")
        
        # 4. å½“å‰ä»»åŠ¡
        context_parts.append("Here are the task:")
        context_parts.append(message)
        
        return "\n".join(context_parts)
    
    def _generate_response(self, context: str, messages: List[Dict]) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆå›å¤ã€‚"""
        # å¯¼å…¥ MEMORY_ANSWER_PROMPT
        try:
            from prompts import MEMORY_ANSWER_PROMPT
            system_prompt = f"{MEMORY_ANSWER_PROMPT}\n\nUser ID: {self.current_user_id}\n\n{context}"
        except ImportError:
            system_prompt = f"""You are an AI assistant with long-term memory capabilities. User ID: {self.current_user_id}
Please answer based on the user's messages and relevant memories. If there are relevant memories, reflect that you remember the user's information in your response.
Maintain a friendly and natural conversation style.

{context}"""
        
        try:
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = messages[-1]["content"] if messages else ""
            ai_response = self.memory._llm_client.chat(system_prompt, user_message)
            return ai_response
        except Exception as llm_error:
            return f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤ã€‚é”™è¯¯: {str(llm_error)}"
    
    async def _add_to_memory_async(self, message: str, history: List[Dict[str, str]]) -> None:
        """å¼‚æ­¥æ·»åŠ è®°å¿†åˆ°åå°ï¼ˆä¸é˜»å¡ Gradio äº‹ä»¶å¾ªç¯ï¼‰ã€‚"""
        try:
            chat_id = f"chat_{int(time.time())}"
            
            # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ç”¨äºè®°å¿†æå–
            conversation_context = self._build_conversation_context(message, history)
            
            # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œæœªå®ç°æ—¶å›é€€åˆ°çº¿ç¨‹æ± å°è£…çš„åŒæ­¥æ¥å£
            if hasattr(self.memory, "add_async"):
                await self.memory.add_async(
                    text=conversation_context,
                    user_id=self.current_user_id,
                    chat_id=chat_id
                )
            else:
                await asyncio.to_thread(
                    self.memory.add,
                    conversation_context,
                    self.current_user_id,
                    chat_id
                )
        except Exception as e:
            logger.warning(f"Async memory add failed: {e}")

    async def _reconsolidate_async(self, query: str) -> None:
        """å¼‚æ­¥å·©å›ºæ£€ç´¢åˆ°çš„æƒ…æ™¯è®°å¿†ï¼Œé¿å…é˜»å¡å“åº”ã€‚"""
        try:
            if hasattr(self.memory, "reconsolidate_async"):
                await self.memory.reconsolidate_async(query, self.current_user_id)
            else:
                # å›è½ï¼šåœ¨åå°çº¿ç¨‹è°ƒç”¨å¸¦ reconsolidate çš„ search
                await asyncio.to_thread(
                    self.memory.search,
                    query,
                    self.current_user_id,
                    5,
                    True
                )
        except Exception as e:
            logger.warning(f"Async reconsolidation failed: {e}")
    
    async def _intelligent_reconsolidate_async(
        self,
        query: str,
        retrieved_memories: List[MemoryRecord],
        full_context: str,
        final_reply: str,
        history: List[Dict[str, str]]
    ) -> None:
        """åŸºäºå®é™…ä½¿ç”¨æƒ…å†µçš„æ™ºèƒ½è®°å¿†å·©å›ºã€‚
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_memories: æ£€ç´¢åˆ°çš„æ‰€æœ‰è®°å¿†
            full_context: å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ï¼‰
            final_reply: æœ€ç»ˆå›å¤
            history: æ¶ˆæ¯å†å²
        """
        try:
            # å‡†å¤‡åˆ¤æ–­ä¸Šä¸‹æ–‡
            system_prompt, episodic_texts, semantic_texts, message_history, final_reply_text = \
                self._prepare_judgment_context(query, retrieved_memories, final_reply, history)
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ™ºèƒ½å·©å›º
            await asyncio.to_thread(
                self.memory._intelligent_reconsolidate,
                query,
                retrieved_memories,
                system_prompt,
                message_history,
                final_reply_text
            )
        except Exception as e:
            logger.warning(f"Intelligent reconsolidation failed: {e}")
    
    def _prepare_judgment_context(
        self,
        query: str,
        retrieved_memories: List[MemoryRecord],
        final_reply: str,
        history: List[Dict[str, str]]
    ) -> Tuple[str, List[str], List[str], List[Dict[str, str]], str]:
        """ä¸ºè®°å¿†ä½¿ç”¨åˆ¤æ–­å‡†å¤‡å®Œæ•´ä¸Šä¸‹æ–‡ã€‚
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_memories: æ£€ç´¢åˆ°çš„è®°å¿†
            final_reply: æœ€ç»ˆå›å¤
            history: æ¶ˆæ¯å†å²
            
        Returns:
            (system_prompt, episodic_texts, semantic_texts, message_history, final_reply_text)
        """
        # 1. è·å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = self._get_system_prompt()
        
        # 2. åˆ†ç¦»è®°å¿†
        episodic_texts = [mem.text for mem in retrieved_memories if mem.memory_type == "episodic"]
        semantic_texts = [mem.text for mem in retrieved_memories if mem.memory_type == "semantic"]
        
        # 3. å‡†å¤‡æ¶ˆæ¯å†å²
        message_history = history
        
        # 4. æœ€ç»ˆå›å¤
        final_reply_text = final_reply
        
        return system_prompt, episodic_texts, semantic_texts, message_history, final_reply_text
    
    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯ã€‚"""
        try:
            from prompts import MEMORY_ANSWER_PROMPT
            return f"{MEMORY_ANSWER_PROMPT}\n\nUser ID: {self.current_user_id}"
        except ImportError:
            return f"You are an AI assistant with long-term memory capabilities. User ID: {self.current_user_id}"
    
    def _build_conversation_context(self, message: str, history: List[Dict[str, str]]) -> str:
        """æ„å»ºç”¨äºè®°å¿†æå–çš„å¯¹è¯ä¸Šä¸‹æ–‡ã€‚"""
        # åŒ…å«æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆæœ€å¤š3è½®ï¼‰
        context_parts = []
        history_pairs = self._history_pairs(history)
        
        for user_msg, ai_msg in history_pairs[-3:]:
            context_parts.append(f"ç”¨æˆ·: {user_msg}")
            context_parts.append(f"åŠ©æ‰‹: {ai_msg}")
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        context_parts.append(f"ç”¨æˆ·: {message}")
        
        return "\n".join(context_parts)
    
    async def run_consolidation(self, progress=gr.Progress()) -> Tuple[str, str]:
        """Run memory consolidation with progress updates."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ", await asyncio.to_thread(self.get_all_memories)
        
        if self.is_consolidating:
            return "â³ å·©å›ºä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­...", await asyncio.to_thread(self.get_all_memories)
        
        self.is_consolidating = True
        self.consolidation_log = []
        
        try:
            self.consolidation_log.append(f"ğŸš€ å¼€å§‹å·©å›º - {datetime.now().strftime('%H:%M:%S')}")
            progress(0.1, desc="æ­£åœ¨æŸ¥è¯¢è®°å¿†...")
            
            # Run consolidation in a worker thread to keep UI responsive
            stats = await asyncio.to_thread(self.memory.consolidate, user_id=self.current_user_id)
            
            progress(0.9, desc="å·©å›ºå®Œæˆ")
            
            # Build result log
            log = []
            log.append(f"âœ… å·©å›ºå®Œæˆ - {datetime.now().strftime('%H:%M:%S')}")
            log.append(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            log.append(f"  - å¤„ç†è®°å¿†æ•°: {stats.memories_processed}")
            log.append(f"  - åˆ›å»ºè¯­ä¹‰æ•°: {stats.semantic_created}")
            
            self.consolidation_log.extend(log)
            progress(1.0, desc="å®Œæˆ")
            
            return "\n".join(self.consolidation_log), await asyncio.to_thread(self.get_all_memories)
            
        except Exception as e:
            error = f"âŒ å·©å›ºå¤±è´¥: {str(e)}"
            self.consolidation_log.append(error)
            return "\n".join(self.consolidation_log), await asyncio.to_thread(self.get_all_memories)
        finally:
            self.is_consolidating = False
    
    async def reset_memories(self) -> Tuple[str, str]:
        """Reset all memories for current user."""
        if not self.memory:
            return "âš ï¸ è¯·å…ˆåˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ", ""
        
        try:
            count = await asyncio.to_thread(self.memory.reset, self.current_user_id)
            self.chat_history = []
            return f"âœ… å·²åˆ é™¤ {count} æ¡è®°å¿†", await asyncio.to_thread(self.get_all_memories)
        except Exception as e:
            return f"âŒ é‡ç½®å¤±è´¥: {str(e)}", await asyncio.to_thread(self.get_all_memories)


def create_demo_interface():
    """Create and return the Gradio interface."""
    app = MemoryDemoApp()
    
    with gr.Blocks(title="AI Memory System Demo", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ§  AI Memory System å¯è§†åŒ–æµ‹è¯•")
        gr.Markdown("åŸºäºè®¤çŸ¥å¿ƒç†å­¦çš„AIé•¿æœŸè®°å¿†ç³»ç»Ÿæ¼”ç¤º")
        
        with gr.Row():
            user_id_input = gr.Textbox(label="ç”¨æˆ·ID", value="demo_user", scale=2)
            init_btn = gr.Button("ğŸ”„ åˆå§‹åŒ–ç³»ç»Ÿ", variant="primary", scale=1)
            init_status = gr.Textbox(label="çŠ¶æ€", interactive=False, scale=2)
        
        with gr.Row():
            # Left panel - Memory display
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“š è®°å¿†åº“")
                memory_display = gr.Textbox(
                    label="å®æ—¶è®°å¿†çŠ¶æ€",
                    lines=25,
                    max_lines=30,
                    interactive=False
                )
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°è®°å¿†", variant="secondary")
                
                gr.Markdown("### âš™ï¸ è®°å¿†å·©å›º")
                consolidate_btn = gr.Button("ğŸ”§ è¿è¡Œå·©å›º", variant="primary")
                consolidation_output = gr.Textbox(label="å·©å›ºæ—¥å¿—", lines=8, interactive=False)
                
                reset_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºè®°å¿†", variant="stop")
                reset_output = gr.Textbox(label="æ“ä½œç»“æœ", lines=2, interactive=False)
            
            # Right panel - Chat interface
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ’¬ å¯¹è¯æµ‹è¯•")
                chatbot = gr.Chatbot(label="å¯¹è¯å†å²", height=400, type='messages')
                msg_input = gr.Textbox(label="è¾“å…¥æ¶ˆæ¯", placeholder="è¾“å…¥è¦è®°å¿†çš„å†…å®¹...")
                send_btn = gr.Button("å‘é€", variant="primary")
                
                gr.Markdown("### ğŸ’¡ æµ‹è¯•å»ºè®®")
                gr.Markdown("""
                - è¾“å…¥ä¸ªäººä¿¡æ¯: "æˆ‘æ˜¯åŒ—äº¬å¤§å­¦è®¡ç®—æœºä¸“ä¸šçš„å­¦ç”Ÿ"
                - æ˜ç¡®è®°å¿†è¯·æ±‚: "è¯·è®°ä½æˆ‘å–œæ¬¢å–å’–å•¡"
                - é¡¹ç›®ä¿¡æ¯: "æˆ‘æ­£åœ¨å¼€å‘ä¸€ä¸ªAIè®°å¿†ç³»ç»Ÿ"
                - é—²èŠæµ‹è¯•: "ä½ å¥½" (ä¸ä¼šè¢«è®°å½•)
                """)
        
        # Event handlers
        init_btn.click(
            fn=app.initialize_memory_system,
            inputs=[user_id_input],
            outputs=[init_status]
        ).then(
            fn=app.get_all_memories,
            outputs=[memory_display]
        )
        
        refresh_btn.click(fn=app.get_all_memories, outputs=[memory_display])
        
        send_btn.click(
            fn=app.chat,
            inputs=[msg_input, chatbot],
            outputs=[msg_input, chatbot, memory_display]
        )
        
        msg_input.submit(
            fn=app.chat,
            inputs=[msg_input, chatbot],
            outputs=[msg_input, chatbot, memory_display]
        )
        
        consolidate_btn.click(
            fn=app.run_consolidation,
            outputs=[consolidation_output, memory_display]
        )
        
        reset_btn.click(
            fn=app.reset_memories,
            outputs=[reset_output, memory_display]
        )
    
    return demo


if __name__ == "__main__":
    demo = create_demo_interface()
    demo.launch(server_name="0.0.0.0", server_port=7861, share=False)
