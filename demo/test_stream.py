#!/usr/bin/env python3
"""
æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½çš„ç®€å•è„šæœ¬
"""

import asyncio
import sys
import os

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.memory_system.clients.llm import LLMClient
from src.memory_system.config import Config

async def test_streaming():
    """æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½...")
    
    # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    config = Config()
    llm_client = LLMClient(
        api_key=config.deepseek_api_key,
        base_url=config.deepseek_base_url,
        model=config.deepseek_model
    )
    
    # æµ‹è¯•æ¶ˆæ¯
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"
    user_message = "ä½ å¥½ï¼Œè¯·ç”¨æµå¼æ–¹å¼å›å¤æˆ‘ã€‚"
    
    print(f"ğŸ“ ç”¨æˆ·æ¶ˆæ¯: {user_message}")
    print("ğŸ¤– AIå›å¤ï¼ˆæµå¼ï¼‰: ", end="", flush=True)
    
    try:
        # æµ‹è¯•æµå¼è¾“å‡º
        response_text = ""
        for chunk in llm_client.chat_stream(system_prompt, user_message):
            print(chunk, end="", flush=True)
            response_text += chunk
        
        print("\n\nâœ… æµå¼è¾“å‡ºæµ‹è¯•æˆåŠŸï¼")
        print(f"ğŸ“Š å®Œæ•´å›å¤é•¿åº¦: {len(response_text)} å­—ç¬¦")
        
        # æµ‹è¯•éæµå¼è¾“å‡ºå¯¹æ¯”
        print("\nğŸ”„ æµ‹è¯•éæµå¼è¾“å‡ºå¯¹æ¯”...")
        normal_response = llm_client.chat(system_prompt, user_message)
        print(f"ğŸ“Š éæµå¼å›å¤é•¿åº¦: {len(normal_response)} å­—ç¬¦")
        
        if response_text.strip() == normal_response.strip():
            print("âœ… æµå¼å’Œéæµå¼ç»“æœä¸€è‡´ï¼")
        else:
            print("âš ï¸ æµå¼å’Œéæµå¼ç»“æœä¸ä¸€è‡´ï¼Œéœ€è¦æ£€æŸ¥")
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = asyncio.run(test_streaming())
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)
